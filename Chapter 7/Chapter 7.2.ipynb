{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7.2 Hierarchical Clustering\n",
    "\n",
    "In this section, we will learn another clustering algorithm called **hierarchical clustering**. Unlike with $k$-means, the number of clusters does not need to be prespecified in hierarchical clustering. It will learn an entire _hierarchy_ of models: a 1-cluster model, a 2-cluster model, and so on, up to an $n$-cluster model (i.e., where each of the $n$ observations are in their own cluster). This hierarchy has the property that the $(k+1)$-cluster model is obtained by splitting one of the clusters in the $k$-cluster model.\n",
    "\n",
    "How do we learn a hierarchy of clusters? There are two general approaches:\n",
    "\n",
    "- **Divisive** (\"top-down\"): Start with all of the observations in a single cluster. Recursively split clusters into smaller clusters, until every observation is in its own cluster.\n",
    "- **Agglomerative** (\"bottom-up\"): Start with each observation in its own cluster. Recursively merge clusters into larger clusters, until every observation is in a single cluster.\n",
    "\n",
    "In this section, we will focus on _agglomerative_ hierarchical clustering algorithms, which are more commonly used and easier to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm, in Pictures\n",
    "\n",
    "Suppose we have a dataset consisting of the 5 points shown below.\n",
    "\n",
    "<img src=\"hierarchical0.jpg\" width=\"400\" />\n",
    "\n",
    "In agglomerative hierarchical clustering, each point starts out in its own cluster, and we successively merge clusters. The first step is clearly to merge A and B into one cluster, since they are the two closest points.\n",
    "\n",
    "<img src=\"hierarchical1.jpg\" width=\"400\" />\n",
    "\n",
    "We now have 4 clusters: {A, B}, {C}, {D}, {E}. What do we merge next? Here things become slightly tricky. So far, we have only defined the distance between two _points_. But now we need a way to measure the distance between a _cluster_ and a _point_, and more generally, the distance between two _clusters_. It turns out that there are many ways to extend a distance metric between points to a distance metric between clusters. These different ways are called **linkages**. The choice of linkage can have a significant influence on the clusters that are obtained. We will return to this point later.\n",
    "\n",
    "For now, it seems that no matter how you look at it, the \"closest\" clusters are clearly {C} and {A, B}, so let's merge them.\n",
    "\n",
    "<img src=\"hierarchical2.jpg\" width=\"400\" />\n",
    "\n",
    "Next, we merge D and E into one cluster.\n",
    "\n",
    "<img src=\"hierarchical3.jpg\" width=\"400\" />\n",
    "\n",
    "And finally, we merge the two clusters into one large cluster containing all 5 observations.\n",
    "\n",
    "<img src=\"hierarchical4.jpg\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dendrograms: A Way to Visualize Hierarchical Clustering\n",
    "\n",
    "**Dendrograms** (from the Greek word _dendron_, meaning \"tree\") are a way to visualize a hierarchy of clusters. The individual observations are listed at the tips of the branches. Each U-shaped merger of two branches on this diagram corresponds to two clusters merging. The height at which the branches merge corresponds to the distance between the clusters before they merged.\n",
    "\n",
    "<img src=\"dendro1.jpg\" width=\"400\" />\n",
    "\n",
    "Suppose we want two clusters. We would cut the dendrogram at a height where there are exactly two branches remaining. We can see easily from the dendrogram that the resulting clusters are {A, B, C} and {D, E}.\n",
    "\n",
    "<img src=\"dendro2.jpg\" width=\"400\" />\n",
    "\n",
    "Similarly, if we wanted four clusters, we would cut the dendrogram at a height where there are exactly four branches remaining. We can see from the dendrogram that the resulting clusters would be {A, B}, {C}, {D}, {E}.\n",
    "\n",
    "<img src=\"dendro3.jpg\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about Linkages\n",
    "\n",
    "Let's return to the question of how to measure the distance between two clusters. A distance metric for clusters, which we will represent by $D$, is built on top of a distance metric for points, which we will represent by $d$ (which might be, for example, Euclidean distance, taxicab distance, etc.). There are a number of different ways to extend a distance metric for points to a distance metric for clusters. These different ways are called **linkages**. Some common ones are:\n",
    "\n",
    "- **single linkage:** the distance between the two _closest_ points in the clusters\n",
    "\n",
    "$$D(A, B) = \\min\\{ d(a, b): a \\in A, b \\in B \\}$$\n",
    "\n",
    "- **complete linkage:** the distance between the two _furthest_ points in the clusters\n",
    "\n",
    "$$D(A, B) = \\max\\{ d(a, b): a \\in A, b \\in B \\}$$\n",
    "\n",
    "- **average linkage:** the average of all pairwise distances between points in the clusters\n",
    "\n",
    "$$D(A, B) = \\frac{1}{|A| |B|} \\sum_a \\sum_b d(a, b) $$\n",
    "\n",
    "These three linkages are illustrated in the diagrams below.\n",
    "\n",
    "<img src=\"linkage1.jpg\" width=\"400\" /> <img src=\"linkage2.jpg\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering in Scikit-Learn\n",
    "\n",
    "Although hierarchical clustering is intuitive, it is somewhat tricky to implement, so we will not pursue that here. Instead, we will use two professional implementations of hierarchical clustering: one in Scikit-Learn and another in Scipy.\n",
    "\n",
    "We start with Scikit-Learn because its API is probably familiar to you by now. In Scikit-Learn, you choose the number of clusters upfront. Let's look at how to fit a hierarchical clustering model to the same iris dataset from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "iris = pd.read_csv(\"/data301/data/iris.csv\")\n",
    "X_train = iris[[\"petal length\", \"petal width\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "model = AgglomerativeClustering(n_clusters=3, \n",
    "                                affinity=\"euclidean\",\n",
    "                                linkage=\"complete\")\n",
    "model.fit(X_train)\n",
    "clusters = model.labels_\n",
    "\n",
    "clusters = pd.Series(clusters).map({\n",
    "    0: \"r\",\n",
    "    1: \"b\",\n",
    "    2: \"y\"\n",
    "})\n",
    "\n",
    "X_train.plot.scatter(x=\"petal length\", y=\"petal width\", \n",
    "                     c=clusters, marker=\"x\", alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disadvantages of using Scikit-Learn to do hierarchical clustering are: (1) it does not support single linkage (although [it was recently implemented](https://github.com/scikit-learn/scikit-learn/pull/9372) and is now in the dev version), and (2) it does not draw dendrograms. For these features, we need to turn to `scipy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering in Scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scipy` is a Python library for scientific computing. It contains many useful functions for signal processing, optimization, simulation, and clustering. The hierarchical clustering API in `scipy` works as follows:\n",
    "\n",
    "- First, you create a _linkage matrix_ that encodes the clustering.\n",
    "- Then, you call `fcluster()` on this linkage matrix to get the cluster assignments or `dendrogram()` to get a plot of the dendrogram. (This function also returns a bunch of other output which you probably do not need; you can suppress the output by adding a semicolon to the end of the line.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster, linkage, dendrogram\n",
    "\n",
    "Z = linkage(X_train, method=\"complete\", metric=\"euclidean\")\n",
    "clusters = fcluster(Z, 3, criterion=\"maxclust\")\n",
    "\n",
    "clusters = pd.Series(clusters).map({\n",
    "    1: \"r\",\n",
    "    2: \"b\",\n",
    "    3: \"y\"\n",
    "})\n",
    "\n",
    "X_train.plot.scatter(x=\"petal length\", y=\"petal width\", \n",
    "                     c=clusters, marker=\"x\", alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram(Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** Fit a hierarchical clustering model to the Titanic passengers dataset (`/data301/data/titanic.csv`). You are free to choose which features to include (but please include both categorical and quantitative features) and the linkage function. Then, choose a number of clusters that seems appropriate. Look at the profiles of the passengers in each cluster. Can you come up with an \"interpretation\" of each cluster based on the passengers in it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.** The code below reads in the \"two moons\" dataset, a synthetic dataset that is used to evaluate clustering algorithms. What clusters do you think hierarchical clustering will find if you use single linkage? What if you use average linkage? Once you have a hypothesis for each type of linkage, test out your hypothesis by fitting the model to this dataset and plotting the resulting clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE\n",
    "moons = pd.read_csv(\"/data301/data/two_moons.csv\")\n",
    "moons.plot.scatter(x=\"x1\", y=\"x2\", color=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.** The code below reads in the \"satellite\" dataset, a synthetic dataset that is used to evaluate clustering algorithms. What clusters do you think hierarchical clustering will find if you use single linkage? What if you use average linkage? Once you have a hypothesis for each type of linkage, test out your hypothesis by fitting the model to this dataset and plotting the resulting clusters.\n",
    "\n",
    "_Food for thought:_ How do the results here compare to $k$-means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE\n",
    "satellite = pd.read_csv(\"/data301/data/satellite.csv\")\n",
    "satellite.plot.scatter(x=\"x1\", y=\"x2\", color=\"k\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
