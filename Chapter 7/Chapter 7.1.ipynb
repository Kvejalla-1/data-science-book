{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Identifying Similar Groups in Data\n",
    "\n",
    "In many applications, we want to cluster observations into similar groups based on observed features. For example, in marketing, it is common to divide potential customers into groups for the purposes of targeting. (For example, a business might want to target a campaign at a group that is most likely to respond to the campaign.) This practice is known as **market segmentation**.\n",
    "\n",
    "This chapter is about how to automatically identify clusters in data, a problem known as **clustering**. Like the classification problems we examined in the previous chapter, clustering is about dividing observations into categories based on features. Unlike classification, however, we do not have ground truth labels that specify what the categories should be; we have to infer them automatically from data. In other words, with classification, we have both features $X$ and labels $y$; with clustering, we only have the features $X$.\n",
    "\n",
    "For this reason, clustering is an example of an **unsupervised learning** problem, in contrast to the **supervised learning** problems of the previous chapters. This unsupervised / supervised terminology comes from the following analogy.  Imagine a child that is trying to learn the difference between shapes and has several examples of each shape in front of him.\n",
    "\n",
    "<img src=\"shape_sorter.jpg\" />\n",
    "\n",
    "The child may be _supervised_ by an adult who gives the child feedback on each answer: \"Yes, that is a circle....No, that was a square....No, that was a circle.\"  This process is analogous to classification, where the labels in the training data can be used to evaluate how good the model's predictions are.\n",
    "\n",
    "On the other hand, the child may be _unsupervised_ and completely left to his own devices. Eventually, he may figure out that there is something similar about all of the circles that distinguish them from the squares. But he won't know that they are called \"circles\", nor does he have any way of evaluating whether he is right or not. This is the fundamental challenge of clustering.\n",
    "\n",
    "We will practice clustering on a dataset containing measurements of 150 iris flowers, collected by the statistician R. A. Fisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 5\n",
    "\n",
    "iris = pd.read_csv(\"/data301/data/iris.csv\")\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on just two of the variables: the petal length and width. The reason for choosing just two variables is so that we can easily visualize the data. Based on the scatterplot below, how many clusters do you think there are in this data set? Can you invent an algorithm that would automatically identify those clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = iris[[\"petal length\", \"petal width\"]]\n",
    "X_train.plot.scatter(x=\"petal length\", y=\"petal width\", \n",
    "                     c=\"black\", marker=\"x\", alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7.1 $K$-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K$-means is an algorithm for finding clusters in data. The idea behind $k$-means is simple: each cluster has a \"center\" point called the **centroid**, and each observation is associated with the cluster of its closest centroid. The only challenge is finding those centroids. The $k$-means algorithm starts with a random guess for the centroids and iteratively improves them.\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "1. Initialize $k$ centroids at random.\n",
    "2. Assign each point to the cluster of its nearest centroid.\n",
    "3. (After the reassignment, the centroid may no longer be at the center of its cluster.) Recompute each centroid based on the points assigned to its cluster.\n",
    "4. Repeat steps 2 and 3 until no points change clusters.\n",
    "\n",
    "Let's see how this works in code. First, let's sample 3 points from the data at random to serve as the initial centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 3 centroids at random from the data.\n",
    "centroids = X_train.sample(3)\n",
    "\n",
    "# Call the three clusters \"red\", \"blue\", \"yellow\" for convenience.\n",
    "centroids.index = [\"r\", \"b\", \"y\"]\n",
    "\n",
    "# Let's plot these centroids.\n",
    "ax = X_train.plot.scatter(x=\"petal length\", y=\"petal width\", \n",
    "                          c=\"black\", marker=\"x\", alpha=.5)\n",
    "centroids.plot.scatter(x=\"petal length\", y=\"petal width\", \n",
    "                       c=centroids.index, ax=ax)\n",
    "\n",
    "centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we assign each point to the cluster of its nearest centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define a function that finds the closest centroid to \n",
    "# a given observation.\n",
    "def get_closest_centroid(obs):\n",
    "    dists = np.sqrt(((obs - centroids) ** 2).sum(axis=1))\n",
    "    return dists.idxmin()\n",
    "\n",
    "get_closest_centroid(X_train.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the entire data set.\n",
    "clusters = X_train.apply(get_closest_centroid, axis=1)\n",
    "\n",
    "# Plot the cluster assignments.\n",
    "ax = X_train.plot.scatter(x=\"petal length\", y=\"petal width\", \n",
    "                          c=clusters, marker=\"x\", alpha=.5)\n",
    "centroids.plot.scatter(x=\"petal length\", y=\"petal width\", \n",
    "                       c=centroids.index, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some of the centroids are no longer at the center of their clusters. We can fix that by redefining the centroid to be the mean of the points in its cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean length and width for each cluster.\n",
    "centroids = X_train.groupby(clusters).mean()\n",
    "\n",
    "# Let's plot the new centroids.\n",
    "ax = X_train.plot.scatter(x=\"petal length\", y=\"petal width\", \n",
    "                          c=clusters, marker=\"x\", alpha=.5)\n",
    "centroids.plot.scatter(x=\"petal length\", y=\"petal width\", \n",
    "                       c=centroids.index, ax=ax)\n",
    "\n",
    "centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there may be some points that are no longer assigned to their closest centroid, so we have to go back and re-assign clusters. But that may cause the centroids to no longer be at the center of their cluster, so we have to recalculate the centroids. And so on. This process continues until the cluster assignments stop changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign points to their closest centroid.\n",
    "clusters = X_train.apply(get_closest_centroid, axis=1)\n",
    "\n",
    "# Recalculate the centroids based on the clusters.\n",
    "centroids = X_train.groupby(clusters).mean()\n",
    "\n",
    "# Plot.\n",
    "ax = X_train.plot.scatter(x=\"petal length\", y=\"petal width\", \n",
    "                          c=clusters, marker=\"x\", alpha=.5)\n",
    "centroids.plot.scatter(x=\"petal length\", y=\"petal width\", \n",
    "                       c=centroids.index, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the above code over and over until the clusters stop changing. This is the final cluster assignment.\n",
    "\n",
    "It is not so easy to visualize the clusters when there are more than 2 features. But we can wrap the same algorithm inside a loop that continues until the cluster assignments do not change from one step to the next. One of the optional exercises below walks you through such an implementation.\n",
    "\n",
    "However, we rarely need to implement $k$-means from scratch. This is because $k$-means is implemented in Scikit-Learn. The API for Scikit-Learn's $k$-means model is similar to the API for supervised learning models, like $k$-nearest neighbors, except that the `.fit()` method only takes in `X`, not `X` and `y`. This makes sense because in unsupervised learning, there are no ground truth labels `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the centroids and the clusters.\n",
    "centroids = model.cluster_centers_\n",
    "clusters = model.labels_\n",
    "\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the cluster numbers to colors.\n",
    "clusters = pd.Series(clusters).map({\n",
    "    0: \"r\",\n",
    "    1: \"b\",\n",
    "    2: \"y\"\n",
    "})\n",
    "\n",
    "# Plot the data\n",
    "X_train.plot.scatter(x=\"petal length\", y=\"petal width\", \n",
    "                     c=clusters, marker=\"x\", alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call `model.predict()` to get the cluster assignment for a new observation. This will simply assign the new observation to the nearest cluster without recalculating the centroids. (If this observation had been in the training data, then assigning the new observation to a cluster would require recalculating the centroid, which would in turn require reassigning observations to clusters, and so on.)\n",
    "\n",
    "For example, consider a flower whose petal has a length of 5.0 and a width of 0.5. It's obvious which cluster this point should be assigned to. Let's check that this is indeed the case, by calling `.predict()` on our fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([[5.0, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note about Standardization\n",
    "\n",
    "Note that we did not standardize the variables in the example above. Just as with $k$-nearest neighbors, it is generally a good idea to standardize your variables before applying $k$-means. Let's see how different the clusters would be if the variables had been standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(X_train_std)\n",
    "\n",
    "clusters = pd.Series(model.labels_).map({\n",
    "    0: \"r\",\n",
    "    1: \"b\",\n",
    "    2: \"y\"\n",
    "})\n",
    "\n",
    "X_train.plot.scatter(x=\"petal length\", y=\"petal width\", \n",
    "                     c=clusters, marker=\"x\", alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare these clusters to the ones we obtained above, keeping in mind that the coloring of the clusters is arbitrary. (All that matters is which observations are grouped together.) We see that only a handful observations near the boundary actually change cluster.  Standardization turns out not to matter much for the iris dataset because the variables are on similar scales. But in data sets with variables on different scales, standardization is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** Use $k$-means to cluster the wines in the wines dataset into 2 clusters. (Code to read in this dataset has been provided below.) How well do your two clusters correspond to white and red wines? (The way the dataset is read in below, the first 1599 wines are red, and the rest are white.)\n",
    "\n",
    "_Hint:_ Don't forget to standardize your variables first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE.\n",
    "red = pd.read_csv(\"/data301/data/winequality-red.csv\", sep=\";\")\n",
    "white = pd.read_csv(\"/data301/data/winequality-white.csv\", sep=\";\")\n",
    "wines = pd.concat([red, white], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.** Use $k$-means to cluster the Titanic passengers (`/data301/data/titanic.csv`) into $k$ clusters. You are free to choose the number of clusters $k$ and the features to include (but be sure to include both categorical and quantitative features). Look at the profiles of the passengers in each cluster. Can you come up with an \"interpretation\" of each cluster based on the passengers in it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.** The code below reads in the \"two moons\" dataset, a synthetic dataset that is used to evaluate clustering algorithms. What should be the two clusters be _intuitively_? What do you think $k$-means will return as the clusters? Once you have a hypothesis, test it out by fitting the model to this dataset and plotting the resulting clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE\n",
    "moons = pd.read_csv(\"/data301/data/two_moons.csv\")\n",
    "moons.plot.scatter(x=\"x1\", y=\"x2\", color=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.** The code below reads in the \"satellite\" dataset, a synthetic dataset that is used to evaluate clustering algorithms. What should the two clusters be _intuitively_? What will the clusters be if you ask $k$-means to cluster this data into 2 clusters? Once you have a hypothesis, test it out by running $k$-means on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE\n",
    "satellite = pd.read_csv(\"/data301/data/satellite.csv\")\n",
    "satellite.plot.scatter(x=\"x1\", y=\"x2\", color=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge Exercise.** Write a function, `get_kmeans_clusters`, that takes in a `DataFrame` and a number of clusters, and returns a `Series` containing the cluster assignment of each observation (row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmeans_clusters(X_train, k):\n",
    "    raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
