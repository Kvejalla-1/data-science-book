{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. How do I build a model to predict a variable of interest?\n",
    "\n",
    "Prediction problems are ubiquitous in real world applications. For example:\n",
    "\n",
    "- A real estate agent might want to predict the fair price of a home, using features about the home.\n",
    "- A sports bettor might want to predict which team will win the game, using information about the teams.\n",
    "- A historian might want to predict which historical figure wrote an anonymous document, using the words in the document.\n",
    "\n",
    "In each case, we have **features**, such as square footage and number of bedrooms, that we want to use to predict a **label**, such as house price. We can formalize the problem mathematically as follows: let $X$ be the features and $y$ the label; a **predictive model** is a function $f$ that maps $X$ to $y$:\n",
    "\n",
    "$$ f: X \\mapsto y. $$\n",
    "\n",
    "Now suppose we have a new house, with features $X^*$. A predictive model $f$ predicts the price of this house to be $f(X^*)$.\n",
    "\n",
    "How do we come up a predictive model $f$ in the first place? One way is to learn it from existing data, or **training data**. The training data consists of features and the corresponding labels. For example, to build a model to predict the price of a home using the square footage (`Gr Liv Area`), we would need training data like the points shown in black below.\n",
    "\n",
    "<img src=\"predictive_model.png\" />\n",
    "\n",
    "We could then learn a model, $f$, from this training data. For example, one possible predictive model is the red curve shown in the plot. This model was chosen to fit the points in the training data as closely as possible. If we wanted to predict the price of a 2700 square foot home using this model, we would simply evaluate $f(2700)$, which comes out to about \\$300,000. The key thing to note is that $f$ depends on the training data. If the training data changes, then so does $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field that studies how to learn predictive models from data is known as **machine learning**. There are many algorithms for learning predictive models from data, including _linear regression_ (which you may be familiar with from a statistics course), _decision trees_, and _neural networks_. In this chapter, we will focus on one machine learning algorithm called **k-nearest neighbors** that leverages the distance metrics that you learned about in the previous chapter.\n",
    "\n",
    "Predictive models are classified into two types, depending on whether the label $y$ is categorical or quantitative. If the label is categorical, then the model is called a **classifier**. If the label is quantitative, then the model is called a **regressor**. Chapter 5.1 covers classification, while Chapter 5.2 covers regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5.1 K-Nearest Neighbors for Classification\n",
    "\n",
    "_Classification models_ are used to predict labels that are categorical. In this section, we will train a machine learning model to predict the color of a wine (red or white) from its chemical properties. \n",
    "\n",
    "The training data for the red and white wines are stored in separate files (`/data301/data/winequality-red.csv` and `/data301/data/winequality-white.csv`). Let's read in the two datasets, add a column for the color (\"red\" or \"white\"), and combine them into one `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 5\n",
    "\n",
    "reds = pd.read_csv(\"/data301/data/winequality-red.csv\", sep=\";\")\n",
    "whites = pd.read_csv(\"/data301/data/winequality-white.csv\", sep=\";\")\n",
    "\n",
    "reds[\"color\"] = \"red\"\n",
    "whites[\"color\"] = \"white\"\n",
    "\n",
    "wines = pd.concat([reds, whites], ignore_index=True)\n",
    "wines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on just two features for now: volatile acidity and total sulfur dioxide. Let's plot the training data, using color to represent the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = wines[\"color\"].map({\n",
    "    \"red\": \"darkred\",\n",
    "    \"white\": \"gold\"\n",
    "})\n",
    "\n",
    "wines.plot.scatter(x=\"volatile acidity\", y=\"total sulfur dioxide\", c=colors, \n",
    "                   alpha=.3, xlim=(0, 1.6), ylim=(0, 400));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that we have a new wine with volatile acidity .85 and total sulfur dioxide 120, represented by a black circle in the plot below. Is this likely a red wine or a white wine?\n",
    "\n",
    "<img src=\"classification.png\" />\n",
    "\n",
    "It is probably not hard to see that this wine is probably red, just by looking at the plot. But let's try to generalize this reasoning into a algorithm that will work when there are more than two features, where visualizing the data is not easy.\n",
    "\n",
    "Your reasoning likely went something like this: most of the wines in the training data that were \"close\" to this wine were reds, so it makes sense to predict that this wine is also red. This is precisely the idea behind the $k$-nearest neighbors algorithm:\n",
    "\n",
    "1. Calculate the distance between the new point and each point in the training data, using some distance metric on the features.\n",
    "2. Determine the $k$ closest points. Of these $k$ closest points, count up how many of each class label there are.\n",
    "3. The predicted class of the new point is whichever class was most common among the $k$ closest points.\n",
    "\n",
    "Let's implement $9$-nearest neighbors for the wine above. First, we extract the training data and standardize the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = wines[[\"volatile acidity\", \"total sulfur dioxide\"]]\n",
    "y_train = wines[\"color\"]\n",
    "\n",
    "X_train_stand = (X_train - X_train.mean()) / X_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create a `Series` for the new wine, making sure to standardize it in the exact same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = pd.Series()\n",
    "x_new[\"volatile acidity\"] = .85\n",
    "x_new[\"total sulfur dioxide\"] = 120\n",
    "\n",
    "x_new_stand = (x_new - X_train.mean()) / X_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the (Euclidean) distance between this new wine and each wine in the training data, and sort the distances from smallest to largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = np.sqrt(((X_train_stand - x_new_stand) ** 2).sum(axis=1))\n",
    "dists_sorted = dists.sort_values()\n",
    "dists_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 9 entries of this sorted list will be the 9 nearest neighbors. Let's get their index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_nearest = dists_sorted.index[:9]\n",
    "inds_nearest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look up these indices in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines.loc[inds_nearest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, notice that these wines are all similar to the new wine in terms of volatile acidity and total sulfur dioxide. To make a prediction for this new wine, we need to count up how many reds and whites there are among these 9-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines.loc[inds_nearest, \"color\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were more reds than whites, so this 9-nearest neighbors model predicts that the wine is red.\n",
    "\n",
    "As a measure of confidence in a prediction, classification models typically report the predicted _probability_ of each label, instead of just the predicted label. The predicted probability of a class in a $k$-nearest neighbors model is simply the proportion of the $k$ neighbors that are in that class. In the above example, instead of simply predicting that the wine is red, we could have instead said that the wine has a $6/9 = .667$ predicted probability of being red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The K-Nearest Neighbors Classification Function\n",
    "\n",
    "We defined a predictive model as a function $f: X \\mapsto y$. What does $f$ look like for a $k$-nearest neighbors classifier? To specify $f$, we have to specify the predicted class for every possible combination of features $X$. When there are only two features $X_1$ and $X_2$, as in the example above, we can use a heat map to represent the predicted class for every possible value of $(X_1, X_2)$.\n",
    "\n",
    "To be concrete, suppose we are predicting the color of a wine from just its volatile acidity and total sulfur dioxide. We have already extracted the training data (`X_train`, `y_train`) above. We need to create a grid of new $X$ values at which to evaluate the predictive model. The code below creates a new `DataFrame` containing a grid of values of volatile acidity and total sulfur dioxide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volatile_acidity = []\n",
    "total_sulfur_dioxide = []\n",
    "for x1 in np.arange(0, 1.7, .05):\n",
    "    for x2 in np.arange(0, 401, 5):\n",
    "        volatile_acidity.append(x1)\n",
    "        total_sulfur_dioxide.append(x2)\n",
    "\n",
    "X_new = pd.DataFrame()\n",
    "X_new[\"volatile acidity\"] = volatile_acidity\n",
    "X_new[\"total sulfur dioxide\"] = total_sulfur_dioxide\n",
    "\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a function `k_nearest_neighbor_class` that implements the $k$-nearest neighbor algorithm above: given a new (standardized) observation, it returns the most common class label among the 9 nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbor_class(x_new_stand):\n",
    "    \"\"\"Given a new observation, returns the k-nearest neighbors prediction\n",
    "    \"\"\"\n",
    "    dists = ((x_new_stand - X_train_stand) ** 2).sum(axis=1)\n",
    "    inds_sorted = dists.sort_values().index[:9]\n",
    "    return y_train.loc[inds_sorted].value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually have 1377 new observations in `X_new`. We'll first standardize this data in the same way that we standardized the training data and then apply the function above to each of the standardized observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_stand = (X_new - X_train.mean()) / X_train.std()\n",
    "predictions = X_new_stand.apply(k_nearest_neighbor_class, axis=1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot these predictions, we'll create a new `DataFrame` that stores both `X_new` and the corresponding predictions. Then, we can make a heatmap of this data using the `.plot.hexbin()` method. This method requires that the labels be converted to a quantitative scale, so we encode \"red\" as 1 and \"yellow\" as 0 and use a colormap that goes from yellow to red. (A list of all of the available colormaps can be found [here](https://matplotlib.org/examples/color/colormaps_reference.html).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = X_new.copy()\n",
    "data_new[\"predictions\"] = 1. * (predictions == \"red\")\n",
    "\n",
    "data_new.plot.hexbin(x=\"volatile acidity\", y=\"total sulfur dioxide\",\n",
    "                     C=\"predictions\", cmap=\"YlOrRd\",\n",
    "                     reduce_C_function=min,\n",
    "                     xlim=(0, 1.6), ylim=(0, 400), gridsize=30,\n",
    "                     sharex=False)\n",
    "# sharex=False is a workaround for a Pandas bug\n",
    "# (https://github.com/pandas-dev/pandas/issues/10678)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** In the above example, we built a 9-nearest neighbors model to predict the color of a wine from just its volatile acidity and total sulfur dioxide. Use the model above to predict the color of the wine below.\n",
    "\n",
    "Now, build a 9-nearest neighbors model using all of the features in the data set. Use this new model to predict the color of the wine below. Does the predicted label change? Do the predicted probabilities of the labels change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE\n",
    "x_new = pd.Series()\n",
    "x_new[\"fixed acidity\"] = 11.2\n",
    "x_new[\"volatile acidity\"] = 0.28\n",
    "x_new[\"citric acid\"] = 0.26\n",
    "x_new[\"residual sugar\"] = 1.9\n",
    "x_new[\"chlorides\"] = 0.075\n",
    "x_new[\"free sulfur dioxide\"] = 17\n",
    "x_new[\"total sulfur dioxide\"] = 60\n",
    "x_new[\"density\"] = 0.998\n",
    "x_new[\"pH\"] = 3.16\n",
    "x_new[\"sulphates\"] = 0.58\n",
    "x_new[\"alcohol\"] = 9.8\n",
    "x_new[\"quality\"] = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.** Build a 5-nearest neighbors model to predict whether or not a passenger on a Titanic would survive, using their age, sex, and class as features. Use the Titanic data set (`/data301/data/titanic.csv`) as your training data. Then, using your model, predict whether a 20-year old female in first-class would survive. What about a 20-year old female in third-class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
