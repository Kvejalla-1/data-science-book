{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5.4 Evaluating Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference between evaluating regression models and evaluating classification models is the choice of error metric. Everything else is the same once you choose an error metric; you can calculate the metric on the training data or on a test set. You can also do cross validation.\n",
    "\n",
    "In this section, we demonstrate how to calculate the metrics on the training data, but you can calculate the same metrics on the test set if you first split your data into a training and a test set, like we did in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's train a $9$-nearest neighbors model on the wine data to predict the wine color and get the predictions on the training data, just to use as an example. The following code is copied from Section 5.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4              0.70         0.00             1.9      0.076   \n",
       "1               7.8              0.88         0.00             2.6      0.098   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "6495            5.5              0.29         0.30             1.1      0.022   \n",
       "6496            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "6495                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "6496                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  color  \n",
       "0         9.4        5    red  \n",
       "1         9.8        5    red  \n",
       "...       ...      ...    ...  \n",
       "6495     12.8        7  white  \n",
       "6496     11.8        6  white  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 5\n",
    "\n",
    "reds = pd.read_csv(\"/data301/data/winequality-red.csv\", sep=\";\")\n",
    "whites = pd.read_csv(\"/data301/data/winequality-white.csv\", sep=\";\")\n",
    "\n",
    "reds[\"color\"] = \"red\"\n",
    "whites[\"color\"] = \"white\"\n",
    "\n",
    "wines = pd.concat([reds, whites], ignore_index=True)\n",
    "wines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = wines[[\"volatile acidity\", \"total sulfur dioxide\"]]\n",
    "y_train = wines[\"color\"]\n",
    "\n",
    "X_train_std = (X_train - X_train.mean()) / X_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_9NN_prediction(obs):\n",
    "    \"\"\"Given a new observation (standardized), find its\n",
    "       9-nearest neighbors in the training data and \n",
    "       return the most common label.\n",
    "    \"\"\"\n",
    "    dists = ((obs - X_train_std) ** 2).sum(axis=1)\n",
    "    i_sorted = dists.sort_values().index[:9]\n",
    "    return y_train.loc[i_sorted].value_counts().idxmax()\n",
    "\n",
    "# Get the prediction for each observation in the training data.\n",
    "y_train_pred = X_train_std.apply(get_9NN_prediction, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for Classification\n",
    "\n",
    "Because the labels $y_i$ in a classification model are categorical, it does not make sense to calculate the difference $y_i - \\hat y_i$ between the actual and predicted labels. But we can determine if the predicted label $\\hat y_i$ is correct ($\\hat y_i = y_i$) or incorrect ($\\hat y_i \\neq y_i$). For example, the **error rate** is defined to be:\n",
    "\n",
    "$$ \\textrm{error rate} = \\textrm{proportion where } \\hat y_i \\neq y_i $$\n",
    "\n",
    "With classification models, it is more common to report the performance in terms of positive metrics, like **accuracy**, where a higher value is better, instead of negative metrics, like error.\n",
    "\n",
    "$$ \\textrm{accuracy} = \\textrm{proportion where } \\hat y_i = y_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96090503309219644"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (y_train_pred == y_train).mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with accuracy is that it is sensitive to the distribution of classes in the training data. For example, if we have a data set with 1% red wines and 99% white wines, it would be trivial to obtain a model with 99% training accuracy: we could simply have a model that predicts that every wine is white. The problem with such a model, despite its high overall accuracy, is that it is remarkably inaccurate for red wines. We need some way to measure \"accuracy\" for a specific class.\n",
    "\n",
    "Suppose we want to know the \"accuracy\" of our model for class $k$. There are two ways to interpret \"accuracy for class $k$\". Do we want to know the accuracy among observations our model _predicted to be_ in class $k$ or the accuracy among observations that _actually were_ in class $k$? The two options lead to two different notions of \"accuracy\" for class $k$: precision and recall.\n",
    "\n",
    "The **precision** of a model for class $k$ is the proportion of observations predicted to be in class $k$ that actually were in class $k$. \n",
    "\n",
    "$$ \\textrm{precision for class } k = \\frac{\\# \\{i:  \\hat y_i = k \\textrm{ and } y_i = k\\}}{\\# \\{i: \\hat y_i = k \\}}. $$\n",
    "\n",
    "The **recall** of a model for class $k$ is the proportion of observations actually in class $k$ that were predicted to be in class $k$.\n",
    "\n",
    "$$ \\textrm{recall for class } k = \\frac{\\# \\{i:  \\hat y_i = k \\textrm{ and } y_i = k\\}}{\\# \\{i: y_i = k \\}}. $$\n",
    "\n",
    "Another way to think about precision and recall is in terms of true positives (TP) and false positives (FP). A \"positive\" is a observation that the model identifies as belonging to class $k$ (i.e., $\\hat y_i = k$). A true positive is one that is actually in class $k$ (i.e., $\\hat y_i = k$ and $y_i = k$), while a false positive is one that is not (i.e., $\\hat y_i = k$ and $y_i \\neq k$). True and false _negatives_ are defined similarly.\n",
    "\n",
    "In this language, the precision is the proportion of positives that are true positives:\n",
    "$$ \\textrm{precision for class } k = \\frac{TP}{TP + FP}, $$\n",
    "while the recall is the proportion of observations in class $k$ that are positives (as opposed to negatives):\n",
    "$$ \\textrm{recall for class } k = \\frac{TP}{TP + FN}. $$\n",
    "\n",
    "A diagram that may help you to remember which numbers go in the numerator and denominator is shown below. The precision is the proportion of the red box that is TP, while the recall is the proportion of the blue circle that is TP.\n",
    "\n",
    "<img src=\"precision-recall.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.92698412698412702, 0.9130706691682301)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = (\n",
    "    ((y_train_pred == \"red\") & (y_train == \"red\")).sum() / \n",
    "    (y_train_pred == \"red\").sum()\n",
    ")\n",
    "recall = (\n",
    "    ((y_train_pred == \"red\") & (y_train == \"red\")).sum() / \n",
    "    (y_train == \"red\").sum()\n",
    ")\n",
    "    \n",
    "precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, there is a tradeoff between precision and recall. For example, you can improve recall by predicting more observations to be in class $k$, but this will hurt precision. To take an extreme example, if you predict that _every_ observation is in class $k$, then your recall would be 100%. But your precision would likely be poor. To see this in the diagram above, notice that you can improve recall by expanding the blue circle, but this may increase the number of false positives and hurt precision.\n",
    "\n",
    "Likewise, you can improve precision by predicting fewer observations to be in class $k$ (i.e., only the ones you are very confident about), but this will hurt recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** In this section, we calculated the _training_ accuracy, precision, and recall for the 9-nearest neighbors classification model. Estimate the _test_ accuracy, precision, and recall for the 9-nearest neighbors model above by first splitting the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.** Build a 5-nearest neighbors model to predict whether or not a passenger on a Titanic would survive, using their age, sex, and class as features. Use the Titanic data set (`/data301/data/titanic.csv`) as your training data. Use cross validation to estimate the test accuracy, as well as the test precision and recall for survivors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
