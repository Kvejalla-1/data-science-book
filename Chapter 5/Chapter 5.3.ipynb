{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5.3 Training Error and Overfitting\n",
    "\n",
    "In the last section, you learned how to build regression models. In this section, you will learn how to evaluate the quality of predictions from a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order</th>\n",
       "      <th>PID</th>\n",
       "      <th>MS SubClass</th>\n",
       "      <th>MS Zoning</th>\n",
       "      <th>Lot Frontage</th>\n",
       "      <th>Lot Area</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>Lot Shape</th>\n",
       "      <th>Land Contour</th>\n",
       "      <th>...</th>\n",
       "      <th>Pool Area</th>\n",
       "      <th>Pool QC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>Misc Feature</th>\n",
       "      <th>Misc Val</th>\n",
       "      <th>Mo Sold</th>\n",
       "      <th>Yr Sold</th>\n",
       "      <th>Sale Type</th>\n",
       "      <th>Sale Condition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>526301100</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>141.0</td>\n",
       "      <td>31770</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>526350040</td>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2928</th>\n",
       "      <td>2929</td>\n",
       "      <td>924100070</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>77.0</td>\n",
       "      <td>10010</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>2930</td>\n",
       "      <td>924151050</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9627</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>188000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2930 rows Ã— 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Order        PID  MS SubClass MS Zoning  Lot Frontage  Lot Area Street  \\\n",
       "0         1  526301100           20        RL         141.0     31770   Pave   \n",
       "1         2  526350040           20        RH          80.0     11622   Pave   \n",
       "...     ...        ...          ...       ...           ...       ...    ...   \n",
       "2928   2929  924100070           20        RL          77.0     10010   Pave   \n",
       "2929   2930  924151050           60        RL          74.0      9627   Pave   \n",
       "\n",
       "     Alley Lot Shape Land Contour    ...     Pool Area Pool QC  Fence  \\\n",
       "0      NaN       IR1          Lvl    ...             0     NaN    NaN   \n",
       "1      NaN       Reg          Lvl    ...             0     NaN  MnPrv   \n",
       "...    ...       ...          ...    ...           ...     ...    ...   \n",
       "2928   NaN       Reg          Lvl    ...             0     NaN    NaN   \n",
       "2929   NaN       Reg          Lvl    ...             0     NaN    NaN   \n",
       "\n",
       "     Misc Feature Misc Val Mo Sold Yr Sold Sale Type  Sale Condition  \\\n",
       "0             NaN        0       5    2010       WD           Normal   \n",
       "1             NaN        0       6    2010       WD           Normal   \n",
       "...           ...      ...     ...     ...       ...             ...   \n",
       "2928          NaN        0       4    2006       WD           Normal   \n",
       "2929          NaN        0      11    2006       WD           Normal   \n",
       "\n",
       "      SalePrice  \n",
       "0        215000  \n",
       "1        105000  \n",
       "...         ...  \n",
       "2928     170000  \n",
       "2929     188000  \n",
       "\n",
       "[2930 rows x 82 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 5\n",
    "\n",
    "housing = pd.read_csv(\"/data301/data/AmesHousing.txt\", sep=\"\\t\")\n",
    "housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics for Regression Models\n",
    "\n",
    "To evaluate the performance of a regression model, we compare the predicted labels from the model against the true labels. Since the labels are quantitative, it makes sense to calculate the difference between each predicted label $\\hat y_i$ and the corresponding true label $y_i$, and then average the squared differences. This measure of error is known as **mean squared error** (or **MSE**, for short):\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\textrm{MSE} &= \\textrm{mean of } (y_i - \\hat y_i)^2.\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "MSE is difficult to interpret because its units are the square of the units of $y$. To make MSE more interpretable, it is common to take the _square root_ of the MSE to obtain the **root mean squared error** (or RMSE, for short):\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\textrm{RMSE} &= \\sqrt{\\textrm{MSE}}.\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "The RMSE measures how off a \"typical\" prediction is.\n",
    "\n",
    "\n",
    "Another common measure of error is the **mean absolute error** (or **MAE**, for short):\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\textrm{MAE} &= \\textrm{mean of } |y_i - \\hat y_i|.\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "Like the RMSE, the MAE measures how off a \"typical\" prediction is. There are other metrics that can be used to measure the quality of a regression model, but these are the most common ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Error\n",
    "\n",
    "To calculate the MSE, RMSE, or MAE, we need data where the true labels are known. Where do we find such data? One natural source of labeled data is the training data, since we need the true labels to be able to train a model.\n",
    "\n",
    "For $k$-nearest neighbors, the training data is the data from which the $k$-nearest neighbors are selected to make predictions. So to calculate the training RMSE, we do the following for each observation in the training data:\n",
    "\n",
    "1. Find its $k$-nearest neighbors in the training data.\n",
    "2. Average the labels of the $k$-nearest neighbors to obtain the predicted label.\n",
    "3. Subtract the predicted label from the true label.\n",
    "\n",
    "Then, we can, for example, square these differences and take their mean to obtain the MSE.\n",
    "\n",
    "Let's calculate the training error for a 10-nearest neighbors model for house price using a subset of features from the Ames housing data set. First, let's get the predicted labels for each house in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features in our model. All quantitative, except Neighborhood.\n",
    "features = [\"Lot Area\", \"Gr Liv Area\",\n",
    "            \"Full Bath\", \"Half Bath\",\n",
    "            \"Bedroom AbvGr\", \n",
    "            \"Year Built\", \"Yr Sold\",\n",
    "            \"Neighborhood\"]\n",
    "\n",
    "X_train = pd.get_dummies(housing[features])\n",
    "y_train = housing[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the variables.\n",
    "X_train_std = (X_train - X_train.mean()) / X_train.std()\n",
    "\n",
    "# Use the average price of the 10 nearest neighbors as the prediction.\n",
    "def get_10NN_prediction(obs):\n",
    "    \"\"\"Given a new observation (standardized), find its\n",
    "       10-nearest neighbors in the training data and \n",
    "       return the average label.\n",
    "    \"\"\"\n",
    "    # Omit the square root, since only the order of the distances matters.\n",
    "    dists = ((obs - X_train_std) ** 2).sum(axis=1)\n",
    "    i_nearest = dists.sort_values().index[:10]\n",
    "    return y_train[i_nearest].mean()\n",
    "\n",
    "# Apply the function above to each row in the training data.\n",
    "y_train_pred = X_train_std.apply(get_10NN_prediction, axis=1)\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compare the predicted labels against the actual labels. Note that the actual labels for the training data are found in `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = ((y_train - y_train_pred) ** 2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number is very large and not very interpretable (because it is in units of \"dollars squared\"). Let's take the square root to obtain the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE says that our model's predictions are, on average, off by about \\$33,000. Not great, but not too bad when an average house is worth about \\$180,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem with Training Error\n",
    "\n",
    "Training error is not a great measure of the quality of a model. To see why, consider a 1-nearest neighbor regression model. Before you read on, can you guess what the training error of a 1-nearest neighbor regression model will be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the price of the 1 nearest neighbor as the prediction.\n",
    "def get_1NN_prediction(obs):\n",
    "    dists = ((obs - X_train_std) ** 2).sum(axis=1)\n",
    "    return y_train[dists.idxmin()]\n",
    "\n",
    "# Apply the function above to each row in the training data.\n",
    "y_train_pred = X_train_std.apply(get_1NN_prediction, axis=1)\n",
    "\n",
    "# Calculate the difference between the predictions and the true labels.\n",
    "mse = ((y_train - y_train_pred) ** 2).mean()\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error of this model seems too good to be true. Can our model really be off by just \\$1000 on average?\n",
    "\n",
    "The error is only small because the nearest neighbor to any point in the training data will always be the point itself! In fact, if we look at the vector of differences between the true and predicted labels, we see that most of the differences were zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train - y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why isn't the MSE exactly equal to 0, then? That is because there may be multiple houses in the training data with the exact same values of the features, so there may be multiple observations that are a distance of 0.0 away. Any one of these observations can be the \"1-nearest neighbor\". If the house that we select to be the nearest neighbor happens to be different from the house we are predicting for, then there is no guarantee that its price will be the same.\n",
    "\n",
    "How many houses did the model get wrong using 1-nearest neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((y_train - y_train_pred) != 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model nailed the price exactly for all but 22 of the 2930 houses. Clearly, training error is too optimistic about the performance of the 1-nearest neighbor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Error\n",
    "\n",
    "The problem with training error is that it reports the error of the model on data that it has already seen. The purpose of building a machine learning model is to predict the labels for new data---data that it did not train on. We would like to measure how well our model would perform on new data. Unfortunately, acquiring new data is expensive. How can we emulate the process of evaluating our model on new data, using just the data that we have?\n",
    "\n",
    "We can split our data into two halves: a **training set** which will be used to train the model and a **test set** which will be used to evaluate the model. To split our data into training and test sets, we can use the `.sample()` function in `pandas`. Let's use this to split our data into two equal halves, which we will call `train` and `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = housing.sample(frac=.5)\n",
    "test = housing.drop(train.index)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use this train/test split to estimate the **test error** of a 10-nearest neighbors model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we extract the variables we need. We have to be careful if we have categorical features because categories that appear in test set may not appear in the training set (and vice versa). To be cautious, we will call `get_dummies` on both sets together and then split them again. Notice the use of `.iloc` below (because our indexes were scrambled by the train/test split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[features]\n",
    "X_test = test[features]\n",
    "\n",
    "y_train = train[\"SalePrice\"]\n",
    "y_test = test[\"SalePrice\"]\n",
    "\n",
    "X = pd.get_dummies(pd.concat([X_train, X_test]))\n",
    "X_train = X.iloc[:len(X_train)]\n",
    "X_test = X.iloc[len(X_train):]\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's standardize the training data. Remember that we have to standardize any new data in exactly the same way, so we need to also standardize the test data using the mean and SD of the _training_ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std = (X_train - X_train.mean()) / X_train.std()\n",
    "X_test_std = (X_test - X_train.mean()) / X_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now in a position to predict on the test set and evaluate the test error. To do this, we need to determine for each observation in the test set, its 10-nearest neighbors in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_10NN_prediction(obs):\n",
    "    \"\"\"Given a new observation (standardized), find its\n",
    "       10-nearest neighbors in the training set and \n",
    "       return the average label.\n",
    "    \"\"\"\n",
    "    dists = ((obs - X_train_std) ** 2).sum(axis=1)\n",
    "    i_nearest = dists.sort_values().index[:10]\n",
    "    return y_train[i_nearest].mean()\n",
    "\n",
    "# Apply the function above to each row in the test set.\n",
    "y_test_pred = X_test_std.apply(get_10NN_prediction, axis=1)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the observations that we are predicting on (i.e., the test set) are completely distinct from the observations that we use to obtain the predictions (i.e., the training set).\n",
    "\n",
    "Finally, let's calculate the test RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = ((y_test - y_test_pred) ** 2).mean()\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the test error is higher than the training error that we calculated earlier. In general, this will be true. It is harder for a model to predict for new observations it has not seen, than for observations it has seen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "One downside of the test error above is that it was calculated using only 50% of the data. As a result, the estimate is very noisy.\n",
    "\n",
    "There is a cheap way to obtain a second opinion of how well our model will do on new data. Previously, we split our data at random into two halves, training the model on the first half and evaluating it using the second half. Because the model has not already seen the second half of the data, this is a valid measure of how well the model would perform on new data. \n",
    "\n",
    "But the way we split our data was arbitrary. We can just as well swap the roles of the two halves, training the model on the _second_ half and evaluating it using the _first_ half. As long as the model is being evaluated on data that is different from the data that was used to train it, we have a valid measure of how well our model would perform on new data. A schematic of this approach, known as **cross-validation**, is shown below.\n",
    "\n",
    "<img src=\"cross-validation.png\" />\n",
    "\n",
    "Because we will be doing the same computations twice, just with different data, let's wrap the $k$-nearest neighbors algorithm above into a function called `get_test_error()`, that computes the test error given training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_error(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # standardize the data with respect to the training set\n",
    "    X_train_std = (X_train - X_train.mean()) / X_train.std()\n",
    "    X_test_std = (X_test - X_train.mean()) / X_train.std()\n",
    "    \n",
    "    # a function that returns the 10NN prediction for a given observation\n",
    "    def get_10NN_prediction(obs):\n",
    "        dists = ((obs - X_train_std) ** 2).sum(axis=1)\n",
    "        i_nearest = dists.sort_values().index[:10]\n",
    "        return y_train[i_nearest].mean()\n",
    "\n",
    "    # get the predictions for the test set\n",
    "    y_test_pred = X_test_std.apply(get_10NN_prediction, axis=1) \n",
    "    \n",
    "    # calculate the RMSE\n",
    "    mse = ((y_test - y_test_pred) ** 2).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply this function to the training and test sets from earlier, we get the same estimate of the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_test_error(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we reverse the roles of the training and test sets, we get another estimate of the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_test_error(X_test, y_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two, somewhat independent estimates of the test error. It is common to average the two to obtain an overall estimate of the test error, called the **cross-validation test error**. Notice that the cross-validation error uses each observation in the data exactly once. We make a prediction for each observation, but always using a model that was trained on data that does not include that observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** Use cross-validation to estimate the test error of a 1-nearest neighbor classifier on the housing price data. How does a 1-nearest neighbor classifier compare to a 10-nearest neighbor classifier in terms of test error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.** Train a $k$-nearest neighbors regression model to predict the tip using the Tips dataset (`/data301/data/tips.csv`). Calculate the training and test errors of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE YOUR CODE HERE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
