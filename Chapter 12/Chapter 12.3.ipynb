{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12.3 Web Scraping\n",
    "\n",
    "**HTML**, which stands for \"hypertext markup language\", is an XML-like language for specifying the appearance of web pages. Each tag in HTML corresponds to a specific page element. For example:\n",
    "\n",
    "- `<img>` specifies an image. The path to the image file is specified in the `src=` attribute.\n",
    "- `<a>` specifies a hyperlink. The text enclosed between `<a>` and `</a>` is the text of the link that appears, while the URL is specified in the `href=` attribute of the tag.\n",
    "- `<table>` specifies a table. The rows of the table are specified by `<tr>` tags nested inside the `<table>` tag, while the cells in each row are specified by `<td>` tags nested inside each `<tr>` tag.\n",
    "\n",
    "Our goal in this section is not to teach you HTML to make a web page. You will learn just enough HTML to be able to scrape data programatically from a web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting HTML Source Code\n",
    "\n",
    "Suppose we want to scrape faculty information from the [Cal Poly Statistics Department directory](https://statistics.calpoly.edu/content/StatisticsDirectory%26Office%20Hours) (`https://statistics.calpoly.edu/content/StatisticsDirectory%26Office%20Hours`). Once we have identified a web page that we want to scrape, the next step is to study the HTML source code. All web browsers have a \"View Source\" or \"Page Source\" feature that will display the HTML source of a web page. \n",
    "\n",
    "Visit the web page above, and view the HTML source of that page. (You may have to do a web search to determine how to view the page source in your favorite browser.) Scroll down until you find the HTML code for the table containing information about the name, office, phone, e-mail, and office hours of the faculty members.\n",
    "\n",
    "Notice how difficult it can be to find a page element in the HTML source. Many browsers allow you to right-click on a page element and jump to the part of the HTML source corresponding to that element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Using BeautifulSoup\n",
    "\n",
    "`BeautifulSoup` is a Python library that makes it easy to navigate an HTML document. Like with `lxml`, we can query tags by name or attribute, and we can narrow our search to the ancestors and descendants of specific tags. In fact, it is possible to use `lxml` with HTML documents, but many web sites have malformed HTML, and `lxml` is not very forgiving. `BeautifulSoup` handles malformed HTML more gracefully and is thus the library of choice.\n",
    "\n",
    "First, we issue an HTTP request to the URL to get the HTML source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "resp = requests.get(\"https://statistics.calpoly.edu/content/StatisticsDirectory%26Office%20Hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTML source is stored in the `.content` attribute of the response object. We pass this HTML source into `BeautifulSoup` to obtain a tree-like representation of the HTML document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(resp.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can search for tags within this HTML document, using tags like `.find_all()`. For example, we can find all tables on this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = soup.find_all(\"table\")\n",
    "len(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a visual inspection of [the web page](https://statistics.calpoly.edu/content/StatisticsDirectory%26Office%20Hours) would confirm, there are 2 tables on the page, and we are interested in the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tables[1]\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one faculty member per row, except for the first row, which is the header. We iterate over all rows except for the first, extracting information about each faculty to append to `rows`, which we will eventually turn into a `DataFrame`. As you read the code below, refer to the HTML source above, so that you understand what each line is doing. In particular, ask yourself the following questions:\n",
    "\n",
    "- `cells[0]` represents a `<td>` tag. Why do we need to call `.find(\"strong\")` within this tag to get the name of the faculty member?\n",
    "- For the most part, `link` is a hyperlink whose text is the faculty's office number. But for some faculty, `link` is `None`. For which faculty is this the case and why?\n",
    "\n",
    "You are encouraged to add `print()` statements inside the `for` loop to check your understanding of each line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for faculty in table.find_all(\"tr\")[1:]:\n",
    "    # Get all the cells in the row.\n",
    "    cells = faculty.find_all(\"td\")\n",
    "    \n",
    "    # The information we need is the text between tags.\n",
    "    name = cells[0].find(\"strong\").text\n",
    "    \n",
    "    link = cells[1].find(\"a\")\n",
    "    office = cells[1].text if link is None else link.text\n",
    "    \n",
    "    email = cells[3].find(\"a\").text\n",
    "    \n",
    "    # Append this data.\n",
    "    rows.append({\n",
    "        \"name\": name,\n",
    "        \"office\": office,\n",
    "        \"email\": email\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, note that `.find_all()` returns a list with all matching tags, while `.find()` returns only the first matching tag. If no matching tags are found, then `.find_all()` will return an empty list `[]`, while `.find()` will return `None`.\n",
    "\n",
    "Finally, we turn `rows` into a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
